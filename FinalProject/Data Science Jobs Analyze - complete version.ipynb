{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Jobs Analyze\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group:\n",
    "* Anchal\n",
    "* Angela\n",
    "* Larissa\n",
    "* Fabio\n",
    "* Felipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program extracts data from Indeed.ca website to analyze main caracteristics \n",
    "of Data Science Job opportunities in Toronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re \n",
    "import requests\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some search Arguments\n",
    "\n",
    "input_job = \"Data Scientist\"\n",
    "# add quotation marks(\"\") to your input_job\n",
    "input_quote = False \n",
    "# leave empty if input_city is not specified\n",
    "input_city = \"\" \n",
    "input_state = \"Canada\"\n",
    "sign = \"+\"\n",
    "#'http://www.indeed.com'\n",
    "BASE_URL_indeed =  'https://ca.indeed.com/' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# Function for Transform searching keywords \n",
    "def transform(input,sign, quote = False):\n",
    "    syntax = input.replace(\" \", sign)\n",
    "    if quote == True:\n",
    "        syntax = ''.join(['\"', syntax, '\"'])\n",
    "    return(syntax)\n",
    "\n",
    "# Funtion to verify a specific Skill(KeyWord) int the Job Requirement attribute (\"big String\")        \n",
    "def find_skill(skill, jobReq ):\n",
    "    if skill in jobReq:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Function to clean text before scraping  \n",
    "def text_cleaner(dirty_text):\n",
    "    text = dirty_text\n",
    "    # break into lines\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) \n",
    "    # Get rid of all blank lines and ends of line\n",
    "    text = ''.join(chunk for chunk in chunks if chunk).encode('utf-8') \n",
    "    # Now clean out all of the unicode junk (this line works great!!!)\n",
    "    try:\n",
    "        # Need this as some websites aren't formatted in a way that this works, can occasionally throw an exception\n",
    "        text = text.decode('unicode_escape').encode('ascii', 'ignore') \n",
    "    except:                                                             \n",
    "        return                                                         \n",
    "   \n",
    "    text = text.decode('utf-8')\n",
    "    # Now get rid of any terms that aren't words (include 3 for d3.js), also include + for C++\n",
    "    text = re.sub(\"[^a-zA-Z+3]\",\" \", text)  \n",
    "    # Fix spacing issue from merged words                                         \n",
    "    text = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", text) \n",
    "    # Go to lower case and split them apart\n",
    "    text = text.lower().split()  \n",
    "    text = list(set(text)) \n",
    "    return text\n",
    "\n",
    "# Web Scraping Attributes\n",
    "\n",
    "# Scraping the Salary\n",
    "def Salary(soup):\n",
    "    try:\n",
    "        Source = soup.find( attrs = {'class' : 'jobsearch-JobMetadataHeader-item'}).text\n",
    "        # print(Source)\n",
    "        Souce_first_letter = Source[0:1]\n",
    "        # print (Souce_first_letter)\n",
    "        if Souce_first_letter == '$':\n",
    "            Salary = Source\n",
    "            return Salary\n",
    "        else:\n",
    "            Salary = 'Not Informed'\n",
    "            return Salary\n",
    "    except:\n",
    "        Salary = 'Not Informed'\n",
    "        return Salary\n",
    "\n",
    "# Scraping the Job Type    \n",
    "def Job_Type(soup):\n",
    "    try:\n",
    "        Source = soup.find( attrs = {'class' : 'jobsearch-JobMetadataHeader-item'}).text\n",
    "        Souce_first_letter = Source[0:1]\n",
    "        if Souce_first_letter != '$':\n",
    "            Job_Type = Source\n",
    "            return Job_Type\n",
    "        else:\n",
    "            Job_Type = 'Not Informed'\n",
    "            return Job_Type\n",
    "    except:\n",
    "        Job_Type = 'Not Informed'\n",
    "        return Job_Type\n",
    "\n",
    "# Scraping the City    \n",
    "def City(soup):\n",
    "    try:\n",
    "        Position = soup.find( attrs = {'class' : 'icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title'})\n",
    "        Title_count = len(Position.text)\n",
    "        Location_2 = soup.find('title')\n",
    "        Final = Location_2.text.find('Indeed')\n",
    "        City = Location_2.text[Title_count+3: Final-3].split(',')[0]\n",
    "        return City\n",
    "    except:\n",
    "        City = 'Not Informed'\n",
    "        return City\n",
    "\n",
    "# Scraping the Province    \n",
    "def Province(soup):\n",
    "    try:\n",
    "        Position = soup.find( attrs = {'class' : 'icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title'})\n",
    "        Title_count = len(Position.text)\n",
    "        Location_2 = soup.find('title')\n",
    "        Final = Location_2.text.find('Indeed')\n",
    "        Province = Location_2.text[Title_count+3: Final-3].split(', ')[1]\n",
    "        return Province\n",
    "    except:\n",
    "        Province = 'Not Informed'\n",
    "        return Province\n",
    "    \n",
    "# Scraping the Position    \n",
    "def Position(soup):\n",
    "    try:\n",
    "        position_1 = soup_html.find(attrs = {'class' : 'icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title'})\n",
    "        Position = position_1.text\n",
    "        return Position\n",
    "    except:\n",
    "        Position = 'Not Informed'\n",
    "        return Position\n",
    "        \n",
    "# Scraping the Job Requirement    \n",
    "def Job_requirements (soup):\n",
    "    try:\n",
    "        General_1 = soup.find( attrs = {'class' : 'jobsearch-JobComponent-description icl-u-xs-mt--md'})\n",
    "        General = General_1.text\n",
    "        return General\n",
    "    except:\n",
    "        General = 'Not Informed' \n",
    "        return General\n",
    "\n",
    "# Scraping the Company Name    \n",
    "def Company_Name(soup):\n",
    "    try:\n",
    "        Company_Name_1 = soup.find( attrs = {'class' : 'icl-u-lg-mr--sm icl-u-xs-mr--xs'})\n",
    "        Company_name = Company_Name_1.text\n",
    "        return Company_name\n",
    "    except:\n",
    "        Company_name = 'Not Informed'\n",
    "        return Company_name       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Base Indeed URL\n",
    "\n",
    "url_indeed_list = [ BASE_URL_indeed, '/jobs?q=', transform(input_job, sign, input_quote), '&l=', input_state]\n",
    "url_indeed = ''.join(url_indeed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the HTML code from the URL\n",
    "rawcode_indeed = requests.get(url_indeed)\n",
    "# Choose \"lxml\" as parser\n",
    "soup_indeed = BeautifulSoup(rawcode_indeed.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of results\n",
    "num_total_indeed = soup_indeed.find(id = 'searchCount').contents[0].split()[-2]\n",
    "# remove non-numeric characters in the string\n",
    "num_total_indeed = re.sub(\"[^0-9]\",\"\", num_total_indeed) \n",
    "num_total_indeed = int(num_total_indeed)\n",
    "# total number of pages\n",
    "num_pages_indeed = int(np.ceil(num_total_indeed/11.0))\n",
    "# the date for today\n",
    "now = datetime.datetime.now()\n",
    "now_str = now.strftime(\"%m/%d/%Y\")\n",
    "now_str_name=now.strftime('%m%d%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping all pages\n",
    "\n",
    "# num_pages_indeed = 5\n",
    "# create an empty dataframe\n",
    "df_base = pd.DataFrame()\n",
    "\n",
    "for i in range(1, num_pages_indeed+1):\n",
    "    # generating the URL\n",
    "    url = ''.join([url_indeed, '&start=', str(i*10)])\n",
    "    # getting the LXML\n",
    "    rawcode = requests.get(url)\n",
    "    soup_lxml = BeautifulSoup(rawcode.text, \"lxml\")\n",
    "    \n",
    "      # pick out all the \"div\" with \"class=\"job-row\"\n",
    "    divs = soup_lxml.findAll(\"div\")\n",
    "    job_divs = [jp for jp in divs if not jp.get('class') is None\n",
    "                    and 'row' in jp.get('class')]\n",
    "    # Looping all jobs for a specific page\n",
    "    for job in job_divs:\n",
    "        try:\n",
    "            # Attribute IX\n",
    "            ix = job.index\n",
    "            # Attribute Job Id\n",
    "            id = job.get('data-jk', None)\n",
    "            # Attribute Link related to job id\n",
    "            link = BASE_URL_indeed + '/viewjob'+ '?jk=' + id\n",
    "            # Parsing the HTML\n",
    "            r = requests.get(link)\n",
    "            soup_html = BeautifulSoup(r.text, \"html.parser\")\n",
    "            # Attribute Job Requirement\n",
    "            jobrequirements = Job_requirements(soup_html)\n",
    "            # Attribute Company Name \n",
    "            company = Company_Name(soup_html)           \n",
    "            # Atribute City\n",
    "            city = City(soup_html)\n",
    "            # Atribute Province\n",
    "            province = Province(soup_html)\n",
    "            # Atribute Salary\n",
    "            salary = Salary(soup_html)\n",
    "           # Attribute Job Type\n",
    "            jobtype = Job_Type(soup_html)\n",
    "            # Attribute Position\n",
    "            position = Position(soup_html)\n",
    "        except:\n",
    "            continue\n",
    "        # Populating the base data frame    \n",
    "        df_base = df_base.append({'ix': ix,\n",
    "                                  'Job_ID': id,\n",
    "                                  'JobTitle': input_job,\n",
    "                                  'JobType':jobtype,\n",
    "                                  'CompanyName': company,\n",
    "                                  'Salary' : salary,\n",
    "                                  'Position':position,\n",
    "                                  'City': city,\n",
    "                                  'Province': province,\n",
    "                                  'JobRequirements': jobrequirements,\n",
    "                                  'Date': now_str,\n",
    "                                  'From':\"Indeed\",\n",
    "                                  'JobLink':'link'},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlating information from the base data frame and keyword table\n",
    "\n",
    "# Reading the data sources skill and kewWord\n",
    "skill_source = pd.read_excel('ds.xlsx', sheet_name='skill')\n",
    "kw = pd.read_excel('ds_kw.xlsx')\n",
    "\n",
    "# Defining a auxiliary data frame skill\n",
    "df_skill_aux = pd.DataFrame(columns=['Job_ID', 'KeyWord', 'Skill_ID'])\n",
    "\n",
    "# Lopping into the base data frame and checking all KeyWords related to the job requiremnt attribute (\"Big String\")\n",
    "z = 0\n",
    "for index, row in df_base.iterrows():\n",
    "    k = kw.loc[kw.JobTitle == row[\"JobTitle\"], ['KeyWord', 'Skill_ID']]\n",
    "    jobReq = row[\"JobRequirements\"]\n",
    "    job_ID = row[\"Job_ID\"]\n",
    "    for idx, skills in k.iterrows():\n",
    "        skill = ' ' + skills[\"KeyWord\"]\n",
    "        skill_ID = skills[\"Skill_ID\"]\n",
    "        func = find_skill(skill, jobReq)\n",
    "        if func:\n",
    "            df_skill_aux.loc[z] = [job_ID, skill, skill_ID]\n",
    "            z += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming and Cleaning the df_tmp to prepare the main data frame.\n",
    "\n",
    "# Creating df_tmp\n",
    "df_tmp = pd.DataFrame()\n",
    "# Merging the Base Data Frame and Skill table\n",
    "df_tmp = df_base.merge(df_skill_aux, how='left', left_on='Job_ID', right_on='Job_ID')\n",
    "# Treating null values\n",
    "values = {'Skill_ID': '-1', 'KeyWord': \"Not found\"}\n",
    "df_tmp = df_tmp.fillna(value=values)\n",
    "# Defing the Skill Id attribute as an Tnteger type\n",
    "df_tmp['Skill_ID'] = df_tmp['Skill_ID'].astype(int)\n",
    "# dropping the Job Requirements colum. This column will not be used for analysis\n",
    "df_tmp.drop(columns=['JobRequirements'])\n",
    "# Preparing the main data frame: Meging the df_tmp and skill source table\n",
    "df_jobs = df_tmp.merge(skill_source, left_on='Skill_ID', right_on='Skill_ID', how='left')\n",
    "# Removing duplicated rows from the main data frame\n",
    "df_jobs = df_jobs.drop_duplicates(subset={'Job_ID','Skill_ID'}, keep='first', inplace=False)\n",
    "# Resetting the data frame index\n",
    "df_jobs = df_jobs.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting files\n",
    "df_base.to_excel('out.xlsx')\n",
    "df_jobs.to_excel('final_out.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the df_jobs to Sqlite DB (jobs table) \n",
    "\n",
    "# import sqlite3\n",
    "# sqlite_file = '/Users/fabio.maia/Downloads/chinook/chinook.db'\n",
    "# conn = sqlite3.connect(sqlite_file)\n",
    "# df = df_jobs[['Job_ID', 'City', 'Province', 'CompanyName', 'Position', 'JobTitle_x', 'JobType', 'Salary', 'KeyWord', 'Skill', 'Category']]\n",
    "# df.to_sql(\"jobs\", conn, if_exists='replace')\n",
    "# conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
