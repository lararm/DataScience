{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science - Extracting Data from Indeed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group:\n",
    "* Anchal\n",
    "* Angela\n",
    "* Larissa\n",
    "* Fabio\n",
    "* Felipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program extracts data from Indeed.ca website to analyze main caracteristics \n",
    "of Data Science Job opportunities in Toronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re \n",
    "import requests\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some search Arguments\n",
    "research_keyword = '\"Data Science\" or \"Data Scientist\"'\n",
    "input_job = \"Data Scientist\"\n",
    "# Add quotation marks(\"\") to your input_job\n",
    "input_quote = False \n",
    "# leave empty if input_city is not specified\n",
    "input_city = \"\" \n",
    "input_state = \"Canada\"\n",
    "sign = \"+\"\n",
    "base_url_indeed =  'https://ca.indeed.com/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "# Function for Transform searching keywords \n",
    "def transform(input,sign, quote = False):\n",
    "    syntax = input.replace(\" \", sign)\n",
    "    if quote == True:\n",
    "        syntax = ''.join(['\"', syntax, '\"'])\n",
    "    return(syntax)\n",
    "\n",
    "# Funtion to verify a specific Skill(KeyWord) int the Job Requirement attribute (\"big String\")        \n",
    "def find_skill(skill, jobReq ):\n",
    "    if \"*\" in skill:\n",
    "        skill = skill.replace('*',' ')\n",
    "\n",
    "    if skill in jobReq:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Web Scraping Attributes\n",
    "# Scraping the Salary\n",
    "def Salary(soup):\n",
    "    try:\n",
    "        source = soup.find( attrs = {'class' : 'jobsearch-JobMetadataHeader-item'}).text\n",
    "        source_first_letter = source[0:1]\n",
    "        if source_first_letter == '$':\n",
    "            salary = source\n",
    "            return salary\n",
    "        else:\n",
    "            salary = 'Not Informed'\n",
    "            return salary\n",
    "    except:\n",
    "        salary = 'Not Informed'\n",
    "        return salary\n",
    "\n",
    "# Scraping the Job Type    \n",
    "def Job_Type(soup):\n",
    "    try:\n",
    "        source = soup.find( attrs = {'class' : 'jobsearch-JobMetadataHeader-item'}).text\n",
    "        source_first_letter = source[0:1]\n",
    "        if source_first_letter != '$':\n",
    "            job_type = source\n",
    "            return job_type\n",
    "        else:\n",
    "            job_type = 'Not Informed'\n",
    "            return job_type\n",
    "    except:\n",
    "        job_type = 'Not Informed'\n",
    "        return job_type\n",
    "\n",
    "# Scraping the City    \n",
    "def City(soup):\n",
    "    try:\n",
    "        position = soup.find( attrs = {'class' : 'icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title'})\n",
    "        title_count = len(position.text)\n",
    "        location_2 = soup.find('title')\n",
    "        final = location_2.text.find('Indeed')\n",
    "        city = location_2.text[title_count+3: final-3].split(',')[0]\n",
    "        return city\n",
    "    except:\n",
    "        city = 'Not Informed'\n",
    "        return city\n",
    "\n",
    "# Scraping the Province    \n",
    "def Province(soup):\n",
    "    try:\n",
    "        position = soup.find( attrs = {'class' : 'icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title'})\n",
    "        title_count = len(position.text)\n",
    "        location_2 = soup.find('title')\n",
    "        final = location_2.text.find('Indeed')\n",
    "        province = location_2.text[title_count+3: final-3].split(', ')[1]\n",
    "        return province\n",
    "    except:\n",
    "        province = 'Not Informed'\n",
    "        return province\n",
    "    \n",
    "# Scraping the Position    \n",
    "def Position(soup):\n",
    "    try:\n",
    "        position_1 = soup_html.find(attrs = {'class' : 'icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title'})\n",
    "        position = position_1.text\n",
    "        return position\n",
    "    except:\n",
    "        position = 'Not Informed'\n",
    "        return position\n",
    "        \n",
    "# Scraping the Job Requirement    \n",
    "def Job_requirements (soup):\n",
    "    try:\n",
    "        general_1 = soup.find( attrs = {'class' : 'jobsearch-JobComponent-description icl-u-xs-mt--md'})\n",
    "        general = general_1.text\n",
    "        return general\n",
    "    except:\n",
    "        general = 'Not Informed' \n",
    "        return general\n",
    "\n",
    "# Scraping the Company Name    \n",
    "def Company_Name(soup):\n",
    "    try:\n",
    "        company_name_1 = soup.find( attrs = {'class' : 'icl-u-lg-mr--sm icl-u-xs-mr--xs'})\n",
    "        company_name = company_name_1.text\n",
    "        return company_name\n",
    "    except:\n",
    "        company_name = 'Not Informed'\n",
    "        return company_name       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Base Indeed URL\n",
    "url_indeed_list = [ base_url_indeed, '/jobs?q=', transform(research_keyword, sign, input_quote), '&l=', input_state]\n",
    "url_indeed = ''.join(url_indeed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the HTML code from the URL\n",
    "rawcode_indeed = requests.get(url_indeed)\n",
    "# Choose \"lxml\" as parser\n",
    "soup_indeed = BeautifulSoup(rawcode_indeed.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998\n"
     ]
    }
   ],
   "source": [
    "# Total number of results\n",
    "num_total_indeed = soup_indeed.find(id = 'searchCount').contents[0].split()[-2]\n",
    "print(num_total_indeed)\n",
    "# Remove non-numeric characters in the string\n",
    "num_total_indeed = re.sub(\"[^0-9]\",\"\", num_total_indeed) \n",
    "num_total_indeed = int(num_total_indeed)\n",
    "# Total number of pages\n",
    "num_pages_indeed = int(np.ceil(num_total_indeed/11.0))\n",
    "# Current date\n",
    "now = datetime.datetime.now()\n",
    "now_str = now.strftime(\"%m/%d/%Y\")\n",
    "now_str_name=now.strftime('%m%d%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping all pages\n",
    "# create an empty dataframe\n",
    "df_base = pd.DataFrame()\n",
    "for i in range(1, num_pages_indeed+1):\n",
    "    # Generating the URL\n",
    "    url = ''.join([url_indeed, '&start=', str(i*10)])\n",
    "    # Getting the LXML\n",
    "    rawcode = requests.get(url)\n",
    "    soup_lxml = BeautifulSoup(rawcode.text, \"lxml\")\n",
    "    # Pick out all the \"div\" with \"class=\"job-row\"\n",
    "    divs = soup_lxml.findAll(\"div\")\n",
    "    job_divs = [jp for jp in divs if not jp.get('class') is None\n",
    "                    and 'row' in jp.get('class')]\n",
    "    # Looping all jobs for a specific page\n",
    "    for job in job_divs:\n",
    "        try:\n",
    "            # Attribute Job Id\n",
    "            id = job.get('data-jk', None)\n",
    "            # Attribute Link related to job id\n",
    "            link = base_url_indeed + '/viewjob'+ '?jk=' + id\n",
    "            # Parsing the HTML\n",
    "            r = requests.get(link)\n",
    "            soup_html = BeautifulSoup(r.text, \"html.parser\")\n",
    "            # Attribute Job Requirement\n",
    "            jobrequirements = Job_requirements(soup_html)\n",
    "            # Attribute Company Name \n",
    "            company = Company_Name(soup_html)           \n",
    "            # Atribute City\n",
    "            city = City(soup_html)\n",
    "            # Atribute Province\n",
    "            province = Province(soup_html)\n",
    "            # Atribute Salary\n",
    "            salary = Salary(soup_html)\n",
    "            # Attribute Job Type\n",
    "            jobtype = Job_Type(soup_html)\n",
    "            # Attribute Position\n",
    "            position = Position(soup_html)\n",
    "        except:\n",
    "            continue\n",
    "        # Populating the base data frame    \n",
    "        df_base = df_base.append({'Job_ID': id,\n",
    "                                  'JobTitle': input_job,\n",
    "                                  'JobType':jobtype,\n",
    "                                  'CompanyName': company,\n",
    "                                  'Salary' : salary,\n",
    "                                  'Position':position,\n",
    "                                  'City': city,\n",
    "                                  'Province': province,\n",
    "                                  'JobRequirements': jobrequirements,\n",
    "                                  'Date': now_str,\n",
    "                                  'From':\"Indeed\",\n",
    "                                  'JobLink': link},\n",
    "                                   ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlating information from the base data frame and keyword table\n",
    "# Reading the data sources skill and kewWord\n",
    "skill_source = pd.read_excel('datasource/ds.xlsx', sheet_name='skill')\n",
    "kw = pd.read_excel('datasource/ds_kw.xlsx')\n",
    "# Defining a auxiliary data frame skill\n",
    "df_skill_aux = pd.DataFrame(columns=['Job_ID', 'KeyWord', 'Skill_ID'])\n",
    "# Lopping into the base data frame and checking all KeyWords related to the job requiremnt attribute (\"Big String\")\n",
    "z = 0\n",
    "for index, row in df_base.iterrows():\n",
    "    k = kw.loc[kw.JobTitle == row[\"JobTitle\"], ['KeyWord', 'Skill_ID']]\n",
    "    jobReq = row[\"JobRequirements\"]\n",
    "    job_ID = row[\"Job_ID\"]\n",
    "    for idx, skills in k.iterrows():\n",
    "#         skill = ' ' + skills[\"KeyWord\"]\n",
    "        skill = skills[\"KeyWord\"]\n",
    "        skill_ID = skills[\"Skill_ID\"]\n",
    "        func = find_skill(skill, jobReq)\n",
    "        if func:\n",
    "            df_skill_aux.loc[z] = [job_ID, skill, skill_ID]\n",
    "            z += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming and Cleaning the df_tmp to prepare the main data frame.\n",
    "# Creating df_tmp\n",
    "df_tmp = pd.DataFrame()\n",
    "# Merging the Base Data Frame and Skill table\n",
    "df_tmp = df_base.merge(df_skill_aux, how='left', left_on='Job_ID', right_on='Job_ID')\n",
    "# Treating null values\n",
    "values = {'Skill_ID': '-1', 'KeyWord': \"Not found\"}\n",
    "df_tmp = df_tmp.fillna(value=values)\n",
    "# Defing the Skill Id attribute as an Tnteger type\n",
    "df_tmp['Skill_ID'] = df_tmp['Skill_ID'].astype(int)\n",
    "# Dropping the Job Requirements colum. This column will not be used for analysis\n",
    "df_tmp.drop(columns=['JobRequirements'])\n",
    "# Preparing the main data frame: Meging the df_tmp and skill source table\n",
    "df_jobs = df_tmp.merge(skill_source, left_on='Skill_ID', right_on='Skill_ID', how='left')\n",
    "# Removing duplicated rows from the main data frame\n",
    "df_jobs = df_jobs.drop_duplicates(subset={'Job_ID','Skill_ID'}, keep='first', inplace=False)\n",
    "# Resetting the data frame index\n",
    "df_jobs = df_jobs.reset_index(drop=True)\n",
    "# df_jobs.head()\n",
    "df_jobs = df_jobs[['Province', 'City', 'Job_ID', 'JobTitle_x', 'Position',  'CompanyName', 'JobType', 'Salary', 'KeyWord', 'Skill', 'Category', 'InUoTProgram']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting files\n",
    "df_base.to_excel('output/df_base_out.xlsx')\n",
    "df_jobs.to_excel('output/df_jobs_out.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exporting the df_jobs to Sqlite DB (jobs table) \n",
    "import sqlite3\n",
    "#sqlite_file =  '/usr/bin/sqlite3'\n",
    "#sqlite_file = '/Users/angelapaiva/Documents/Data Science Course - UoT/FinalProject/job_opp_db.db'\n",
    "#conn = sqlite3.connect(sqlite_file)\n",
    "#df_b = df_base[['Province', 'City', 'Job_ID', 'Position',  'CompanyName', 'JobType', 'Salary', 'JobLink']]\n",
    "#df = df_jobs[['Province', 'City', 'Job_ID', 'JobTitle_x', 'Position',  'CompanyName', 'JobType', 'Salary', 'KeyWord', 'Skill', 'Category', 'InUoTProgram']]\n",
    "#df_b.to_sql(\"jobs_base\", conn, if_exists='replace')\n",
    "#df.to_sql(\"jobs\", conn, if_exists='replace')\n",
    "#conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conn = sqlite3.connect(sqlite_file)\n",
    "#skill_source.to_sql(\"skill\", conn, if_exists='replace')\n",
    "#kw.to_sql(\"kw_skill\", conn, if_exists='replace')\n",
    "#conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
