{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries:\n",
    "#Requests will allow you to send HTTP/1.1 requests using Python. With it, you can add content like headers, form data, multipart files, and parameters via simple Python libraries. It also allows you to access the response data of Python in the same way.\n",
    "#BeautifulSoup\n",
    "\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "URL = \"https://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=Toronto&start=10\"\n",
    "#conducting a request of the stated URL above:\n",
    "page = requests.get(URL)\n",
    "#specifying a desired format of “page” using the html parser - this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "#printing soup in a more structured tree format that makes for easier reading\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_results_per_city = 10\n",
    "#city_set = ['New+York','Chicago','San+Francisco', 'Austin', 'Seattle', 'Los+Angeles', 'Philadelphia', 'Atlanta', 'Dallas','Pittsburgh', 'Portland', 'Phoenix', 'Denver', 'Houston', 'Miami', 'Washington+DC', 'Boulder']\n",
    "city_set = ['Toronto']\n",
    "columns = [\"city\", \"job_title\", \"company_name\", \"location\", \"summary\", \"salary\"]\n",
    "sample_df = pd.DataFrame(columns = columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Stack Developer\n",
      "CareGuide\n",
      "Toronto, ON\n",
      "$50,000 - $100,000 a year\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Larissa\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:146: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
     ]
    }
   ],
   "source": [
    "page = requests.get('https://ca.indeed.com/cmp/Peersight/jobs/Full-Stack-Developer-3c606e32083a32d1?sjdu=vQIlM60yK_PwYat7ToXhk7CMTN6TBlbqwSiHGWKXUH2Blxs3hQ1EwZBhTIDHeY4sOZieL6xhwgV2bWvrltQJnA&tk=1cr5c6gtga0tf803&vjs=3')\n",
    "time.sleep(1)  #ensuring at least 1 second between page grabs\n",
    "soup = BeautifulSoup(page.text, \"lxml\", from_encoding=\"utf-8\")\n",
    "#creating an empty list to hold the data for each posting\n",
    "job_post = [] \n",
    "\n",
    "#Get job title\n",
    "job_title = soup.find(\"h3\", \"jobsearch-JobInfoHeader-title\").text\n",
    "job_post.append(job_title)\n",
    "print(job_title)\n",
    "\n",
    "#Get company name\n",
    "company = soup.find(name=\"div\", attrs={\"class\":\"icl-u-lg-mr--sm icl-u-xs-mr--xs\"}).text\n",
    "job_post.append(company)\n",
    "print(company)\n",
    "\n",
    "#Get location\n",
    "location = soup.find(\"div\", \"jobsearch-InlineCompanyRating\").contents[2].text\n",
    "print(location)\n",
    "\n",
    "#Get salary\n",
    "salary = soup.find(\"div\", \"jobsearch-JobMetadataHeader-item\").text\n",
    "print(salary)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for city in city_set:\n",
    "#     for start in range(0, max_results_per_city, 10):\n",
    "#         page = requests.get('http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=' + str(city) + '&start=' + str(start))\n",
    "#         print(\"page: \" + str(start))\n",
    "#         time.sleep(1)  #ensuring at least 1 second between page grabs\n",
    "#         soup = BeautifulSoup(page.text, \"lxml\", from_encoding=\"utf-8\")\n",
    "#         for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}): \n",
    "#             #specifying row num for index of job posting in dataframe\n",
    "#             num = (len(sample_df) + 1) \n",
    "#             #creating an empty list to hold the data for each posting\n",
    "#             job_post = [] \n",
    "#             #append city name\n",
    "#             job_post.append(city)\n",
    "#             #print(city)\n",
    "            \n",
    "#             #create an empty list with all the urls\n",
    "#             #grab job\n",
    "#             for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
    "#                 job_post.append(a[\"title\"])\n",
    "                \n",
    "#             #grabbing company name\n",
    "#             company = div.find_all(name=\"span\", attrs={\"class\":\"company\"}) \n",
    "#             if len(company) > 0: \n",
    "#                 for b in company:\n",
    "#                     job_post.append(b.text.strip()) \n",
    "#             else: \n",
    "#                 sec_try = div.find_all(name=\"span\", attrs={\"class\":\"result-link-source\"})\n",
    "#                 for span in sec_try:\n",
    "#                     job_post.append(span.text)\n",
    "                    \n",
    "#             #grabbing location name\n",
    "#             c = div.findAll('span', attrs={'class': 'location'})\n",
    "#             if len(c) > 0:\n",
    "#                 for span in c: \n",
    "#                     job_post.append(span.text)\n",
    "#             else:\n",
    "#                     job_post.append(\"Nothing_found\")\n",
    "#             #grabbing summary text\n",
    "#             d = div.findAll('span', attrs={'class': 'summary'}) \n",
    "#             for span in d:\n",
    "#                 job_post.append(span.text.strip()) \n",
    "#             #grabbing salary\n",
    "#             try:\n",
    "#                 job_post.append(div.find('nobr').text) \n",
    "#             except:\n",
    "#                 try:\n",
    "#                     div_two = div.find(name=\"div\", attrs={\"class\":\"sjcl\"}) \n",
    "#                     div_three = div_two.find(\"div\") \n",
    "#                     job_post.append(div_three.text.strip())\n",
    "#                 except:\n",
    "#                     job_post.append(\"Nothing_found\")         \n",
    "                    \n",
    "#             print(\"job post:\")\n",
    "#             print(job_post)\n",
    "#             print(\"job post length:\")\n",
    "#             print(len(job_post))\n",
    "#             sample_df.loc[num] = job_post   \n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
